---
title: "Exercises in learning decision trees and rules in R"
output: html_notebook
---

The aim of the exercise is to illustrate how to use R for performing four important steps of learning predictive models:

1. Cleaning/preprocessing data
2. Estimating the model error
3. Model selection
4. Model inspection and interpretation

The exercises cover two types of predictive models, decision trees and decision rules. 

## Data Set

We are going to learn these predictive models form the data set describing the survival status of the passengers on the [Titanic](https://en.wikipedia.org/wiki/Titanic). The data set is available in the CRAN package [titanic](https://cran.r-project.org/web/packages/titanic/index.html), but we decided to start with reading in the data set from a csv (comma separated values) file, which better correspond to your future tasks of data analysis.

The csv file is available at [https://github.com/ljupco-todorovski/hse-moscow-ml/blob/master/02/titanic.csv](https://github.com/ljupco-todorovski/hse-moscow-ml/blob/master/02/titanic.csv). After the download, put it in the same directory with this notebook.

## Cleaning/Preprocessing Data

While it might be obvious how to import the data set into R using
```{r}
titanic.data = read.table("titanic.csv", sep=",", header=T)
```

The import above is not correct, since the csv file uses the string "?" to indicate missing values. Not that this is an important detail often neglected when importing csv files in R. Having this in mind, the correct way to import the data set is
```{r}
titanic.data = read.table("titanic.csv", sep=",", header=T, na.strings="?")
```

### Cleaning the data frame

After inspecting the resulting data frame, using `summary(titanic.data)` and/or `View(titanic.data)`, one would quickly realize that there are four variables with too many different possible values. These kind of variable does not hold generalization power and we have to remove them from the training data for machine learning. There are four such variables: `x` correspond to a unique passenger ID, `name` to her/his name, `ticket` to the ticket ID, `cabin` to the cabin numbers, and `home.dest` to the passenger home or destination. Thus, we remove the five variables
```{r}
titanic.data$x = NULL
titanic.data$name = NULL
titanic.data$ticket = NULL
titanic.data$cabin = NULL
titanic.data$home.dest = NULL
```

Some machine learning methods (most notably, neural networks) can not handle missing data. To avoid problems with respect to missing data, we are going to remove all the examples containing missing values using
```{r}
titanic.data = na.omit(titanic.data)
```

Finally, note that R might wrongly convert some discrete variables into numeric, merely based on the fact that the discrete values are numbers. To correct for this mistake, we have to convert the numeric variables `pclass` (passinger class) and `survived` (the class, passenger survival status) into factors
```{r}
titanic.data$pclass = factor(titanic.data$pclass, levels=c(1, 2, 3), labels=c("1st", "2nd", "3rd"))
titanic.data$survived = factor(titanic.data$survived, levels=c(0, 1), labels=c("no", "yes"))
```

After the transformations, we can check the dimensions of the cleaned and preprocessed data set using
```{r}
print(sprintf(
  "Data: %d examples, %d attributes, %d classes",
  nrow(titanic.data), ncol(titanic.data) - 1, length(levels(titanic.data$survived))
))
```

### Default accuracy and error

Before learning predictive models, let us check the accuracy of the trivial classifier that for all the examples predicts the majority class, i.e., the class that is most freqently observed in the data
```{r}
titanic.default.accuracy = max(table(titanic.data$survived)) / nrow(titanic.data)
titanic.default.error = 1 - titanic.default.accuracy

print(sprintf("Default accuracy: %.4g", titanic.default.accuracy))
print(sprintf("Default error: %.4g", titanic.default.error))
```


## Decision Trees

For learning decision trees, we are going to use the R implementation of the classical CART (Classification and Regression Trees) algorithm. You will need the libraries [rpart](https://cran.r-project.org/web/packages/rpart/index.html) and [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/index.html) to learn and plot decision trees. Once you have installed the packages, import them using
```{r}
library(rpart)
library(rpart.plot)
```

### Building a single model

To build a single model on the titanic survival data, we use
```{r}
titanic.model = rpart(survived ~ ., data=titanic.data)
```

The first argument to ``rpart`` is a formula ``survived ~ .`` that specifies the class variable to be predicted on the left-hand side and the set of input, predictive variables (or attributes) on the right-hand side. The symbol ``.`` denotes the set of all the other variables in the data except for the class ``survived``. The second parameter specifies the data frame containing the training data table.

We can plot the learned decision tree and inspect its structure with
# Plot (and inspect) the decision tree
```{r}
par(mfrow=c(1,1))
rpart.plot(titanic.model)
```

Furthermore, we can use the `rpart.predict` function to obtain the model predictions on training data and match them against the observed class values to obtain the accuracy and error as follows
```{r}
classification.error = function(true, predicted, print_cm=FALSE) {
  cm = table(true, predicted)
  if (print_cm) {
    print(cm)
  }
  accuracy = sum(diag(cm)) / sum(cm)
  error = 1 - accuracy
  return(error)
}
titanic.model.predictions = rpart.predict(titanic.model, type="class")
titanic.model.error.train = classification.error(titanic.data$survived, titanic.model.predictions, print_cm=TRUE)

print(sprintf("Model error on training data: %.4g", titanic.model.error.train))
```

Note that the function `classification.error` calculates the confusion matrix `cm` first and then uses it to calculate the model accuracy and error. Using the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), one can also measure other aspects of the model predictive performance, such as precision or recall.

Recall from the lectures, that the assessment of the model performance using the training data is naive and overoptimistic. Using training data, we can only estimate model accuracy, but not its generality, i.e., performance on test data unseen when learning the model.

### Estimating the model error with cross-validation

Cross-validation is a standard procedure for out-of-training-data-sample (or simply, out-of-sample) estimates of the performance of predictive models. The idea of cross validation is simple. We first cut the given data set in `k` segments (usually referred to as folds). Then, we run the learning algorithm `k` times: at each run, we train a model on `k - 1` data segments and use it to predict the values of the target variable on the examples from the remaining one. At the end, we have predictions for all the examples in the data. Note however, that all these predictions are obtained with models that have not been learned on the examples they predict.
```{r}
# Cut the vector in npieces pieces and return the list of cuts
cut.in.pieces = function(x, npieces) {
  return(split(x, cut(seq_along(x), npieces, labels = FALSE)))
}

# Return the indexes of examples in individual folds
cross.validation.folds = function(n, k=10, stratification=NULL, seed=NULL) {
  if (!is.null(seed)) {
    set.seed(seed)
  }
  if (is.null(stratification)) {
    cut.in.pieces(sample(1:n), k)
  } else {
    folds = NULL
    for (l in levels(as.factor(stratification))) {
      if (is.null(folds)) {
        folds = cut.in.pieces(sample(which(stratification == l)), k)
      } else {
        folds.l = cut.in.pieces(sample(which(stratification == l)), k)
        for (i in 1:k) {
          folds[[i]] = c(folds[[i]], folds.l[[i]])
        }
      }
    }
  }
  return(folds)
}
```

Before cutting the data into `k` segments, we randomly permute the data samples. When dealing with classification problems, we stratify the segments so they have the similar distribution of the class variable to the whole data set. In the function `cross.validation.folds` above, we do this trick by cutting into pieces set of examples for each class label separately and then join this pieces into folds.

The usual value of `k` is `10`. We usually opt for a smaller value of `k` in cases of small data sets with less than `100` examples.

Once we have the folds, running cross validation is simple
```{r}
titanic.cv = function(folds) {
  cv.predictions = titanic.data$survived
  for (i in 1:5) {
    m.i = rpart(survived ~ ., data=titanic.data[-folds[[i]], ])
    cv.predictions[folds[[i]]] = rpart.predict(m.i, titanic.data[folds[[i]], ], type="class")
  }
  cv.error = classification.error(titanic.data$survived, cv.predictions)
  return(cv.error)
}

cv.folds = cross.validation.folds(nrow(titanic.data), 5, stratification=titanic.data$survived, seed=42)
titanic.model.error.cv = titanic.cv(cv.folds)

print(sprintf("Model error estimated using CV: %.4g", titanic.model.error.cv))
```

Note that we calculated the folds first and then used them for estimation. This comes handy when we plan to use cross validation for model selection (as we do below): then different model variants are being estimated and compared on the same set of folds. Note also that we set the value of the random generator seed to guarantie the reproducibility of the model estimation and selection process.

### Model selection

Recall from the lectures that complexity of a predictive model has a profound impact on its accuracy and generality. We have to take care that we select a model with a right complexity, that makes an optimal trade-off between the train and the test error (or between bias and variance).

We control the complexity of decision tress with pruning. Pre-pruning procedures take care of the pruning within the recursive greedy algorithm for learning trees: once the node has a predefined number of examples `minsplit`, the algorithm turns it into a leaf. Post-pruning procedure runs after we learned the tree and it looks for opportunities to simplify the tree by turning an internal decision node into a leaf. Parameter `cp` controls the trade-off between the tree complexity and its accuracy in the process of post-pruning.

We set the values of these two parameters using the `rpart` parameter `control` and the function`rpart.control` as follows
```{r}
titanic.cv = function(folds, minsplit, cp) {
  cv.predictions = titanic.data$survived
  for (i in 1:5) {
    m.i = rpart(survived ~ ., data=titanic.data[-cv.folds[[i]], ],
                control=rpart.control(minsplit=minsplit, cp=cp)
    )
    cv.predictions[cv.folds[[i]]] = rpart.predict(m.i, titanic.data[cv.folds[[i]], ], type="class")
  }
  cv.error = classification.error(titanic.data$survived, cv.predictions)
  return(cv.error)
}
```

Note that the new version of the `titanic.cv` function accepts two more parameters `minsplit` and `cp` that are being propagated to the algorithm for learning trees.

Now we are ready to perform model selection in order to find optimal settings of the two parameters. First, we select the candidate values of the two parameters and calculate the model performance for each combination of parameter values
```{r}
minsplit.values = c(2, 5, 10, 20, 50)
cp.values = c(0, 0.01, 0.1, 0.2, 0.5, 1)


# Calculate the cross-validated errors of all the decision trees trained with different parameter settings
errors = matrix(0, nrow=length(minsplit.values), ncol=length(cp.values))
for (i in 1:length(minsplit.values)) {
  for (j in 1:length(cp.values)) {
    errors[i, j] = titanic.cv(cv.folds, minsplit.values[i], cp.values[j])
  }
}
```


Find the optimal setting using the `errors` matrix
```{r}
min.error = min(errors)
i.j = which(errors == min.error, arr.ind=TRUE)
min.minsplit = minsplit.values[i.j[1]]
min.cp = cp.values[i.j[2]]

print(sprintf("Optimal setting: minsplit = %d, cp = %g", min.minsplit, min.cp))
print(sprintf("Optimal error: %.4g", min.error))
```

Visualize the model selection results...
```{r}
# FIrst in 2-D considering both parameters simultaneously
library(plot.matrix)

par(mar=c(5.1, 4.1, 4.1, 4.1))
plot(errors, axis.row=list(yaxt="n"), axis.col=list(xaxt="n"),
     xlab="cp", ylab="minsplit", main="titanic: Model Selection (minsplit and cp)",
     border=NA, digits=4, text.cell=list(cex=0.5), col=heat.colors
)
axis(1, at=1:length(cp.values), labels=cp.values)
axis(2, at=length(minsplit.values):1, labels=minsplit.values)

# Then in 1-D considereing each parameter separately
plot(errors[, i.j[2]], type="b", col="red",
     xaxt="n", xlab="minsplit", ylab="error (CV)",
     main=sprintf("titanic: Model Selection (minsplit, cp=%g)", min.cp)
)
axis(1, at=1:length(minsplit.values), labels=minsplit.values)

plot(errors[i.j[1], ], type="b", col="red",
     xaxt="n", xlab="cp", ylab="error (CV)",
     main=sprintf("titanic: Model Selection (cp, minsplit=%d)", min.minsplit)
)
axis(1, at=1:length(cp.values), labels=cp.values)
```

Explain grid search for parameter tuning here and mention the Caret CRAN package that supports paramter tuning.

### Model inspection and interpretation

Use the optimize parameter setting to learn the final decision tree...
```{r}
titanic.model = rpart(survived ~ ., data=titanic.data,
                      control=rpart.control(minsplit=min.minsplit, cp=min.cp)
)
rpart.plot(titanic.model, main="titanic: the Model")
```


The tabular representation of the titanic model can be found in the data frame `titanic.model$frame`, where each row correspond to a single tree node. The column `var` denotes the type of the node (leaf or internal). Using this data frame, we can assess the complexity of the decision tree as follows
```{r}
# Model complexity: number of nodes and leafs
dt.complexity = function(dt) {
  n.leafs = sum(dt$frame$var == "<leaf>")
  n.nodes = nrow(dt$frame)
  return(list(nodes=n.nodes, leafs=n.leafs))
}


dt.c = dt.complexity(titanic.model)
print(sprintf("Number of nodes: %d, leaf nodes %d", dt.c$nodes, dt.c$leafs))
```

We can further use `titanic.model$where` to find training examples that are included in each (leaf) node of the decision tree. Using these data, we can visualize the distribution of the class variable for a given leaf node
```{r}
# Class distributions in individual leaf nodes
dt.leaf.plot = function(dt, y, leaf.index, leaf.description) {
  leaf.examples = which(dt$where == leaf.index)
  leaf.cd = table(y[leaf.examples])
  leaf.cd.pct = round(leaf.cd / sum(leaf.cd) * 100)
  leaf.labels = sprintf("%s-%d%%", names(leaf.cd), leaf.cd.pct)

  all.cd = table(y)
  all.cd.pct = round(all.cd / sum(all.cd) * 100)
  all.labels = sprintf("%s-%d%%", names(all.cd), all.cd.pct)

  par(mar=c(0.1, 2, 4.1, 2))
  par(mfrow=c(1,2))
  pie(all.cd, labels=all.labels, main=sprintf("titanic: all, n=%d", length(y)))
  pie(leaf.cd, labels=leaf.labels,
      main=sprintf("titanic: %s\nn=%d", leaf.description, length(leaf.examples))
  )
  par(mfrow=c(1,1))
}

# Plot the distributions for the two largest leafs in the model
dt.leaf.plot(titanic.model, titanic.data$survived, 3, "sex=male,\nage >= 9.5")
dt.leaf.plot(titanic.model, titanic.data$survived, 15, "sex=female,\nclass != 3rd")
```


## Decision rules

```{r}
# For learning decision rules, we are going to use the CRAN package Rweka
# It allows for using all the algorithms from the Weka workbench for machine learning
# Among them is the algorithm for learning decicion rules RIPPER

library(RWeka)

titanic.cv = function(folds) {
  cv.predictions = titanic.data$survived
  for (i in 1:5) {
    m.i = JRip(survived ~ ., data=titanic.data[-folds[[i]], ])
    cv.predictions[folds[[i]]] = predict(m.i, titanic.data[folds[[i]], ], type="class")
  }
  cv.error = classification.error(titanic.data$survived, cv.predictions)
  return(cv.error)
}
```


### Model selection

```{r}
minweights.values = c(1, 2, 5, 10, 20, 50)
prunning.values = c(TRUE, FALSE)

titanic.cv = function(folds, minweights, prunning) {
  cv.predictions = titanic.data$survived
  for (i in 1:5) {
    m.i = JRip(survived ~ ., data=titanic.data[-cv.folds[[i]], ],
                control=Weka_control(N=minweights, P=prunning)
    )
    cv.predictions[cv.folds[[i]]] = predict(m.i, titanic.data[cv.folds[[i]], ], type="class")
  }
  cv.error = classification.error(titanic.data$survived, cv.predictions)
  return(cv.error)
}

# Calculate the cross-validated errors of all the decision trees trained with different parameter settings
errors = matrix(0, nrow=length(minweights.values), ncol=length(prunning.values))
for (i in 1:length(minweights.values)) {
  for (j in 1:length(prunning.values)) {
    errors[i, j] = titanic.cv(cv.folds, minweights.values[i], prunning.values[j])
  }
}

# Find the minimal error and identify the parameter setting that led to that error
min.error = min(errors)
i.j = which(errors == min.error, arr.ind=TRUE)
min.minweights = minweights.values[i.j[1]]
min.prunning = prunning.values[i.j[2]]

# Plot the results of model selection
# FIrst in 2-D considering both parameters simultaneously
par(mar=c(5.1, 4.1, 4.1, 4.1))
plot(errors, axis.row=list(yaxt="n"), axis.col=list(xaxt="n"),
     xlab="prunning", ylab="minweights", main="titanic: Model Selection (minweigths and prunning)",
     border=NA, digits=4, text.cell=list(cex=0.5), col=heat.colors
)
#axis(1, at=1:length(prunning.values), labels=prunning.values)
axis(1, at=1:length(prunning.values), labels=c("TRUE", "FALSE"))
axis(2, at=length(minweights.values):1, labels=minweights.values)

# Then in 1-D considering each parameter separately
plot(errors[, i.j[2]], type="b", col="red",
     xaxt="n", xlab="minweigths", ylab="error (CV)",
     main=sprintf("titanic: Model Selection (minweigths, prunning=%s)", min.prunning)
)
axis(1, at=1:length(minweights.values), labels=minweights.values)

plot(errors[i.j[1], ], type="b", col="red",
     xaxt="n", xlab="cp", ylab="error (CV)",
     main=sprintf("titanic: Model Selection (prunning, minweights=%d)", min.minweights)
)
#axis(1, at=1:length(prunning.values), labels=prunning.values)
axis(1, at=1:length(prunning.values), labels=c("TRUE", "FALSE"))

print(sprintf("Optimal setting: minweigths = %d, prunning = %s", min.minweights, min.prunning))
print(sprintf("Optimal error: %.4g", min.error))
```

### Model inspection and interpretation

```{r}
# Use the identified parameter setting to learn the final model
titanic.model = JRip(survived ~ ., data=titanic.data,
                      control=Weka_control(N=min.minweights, P=min.prunning)
)
print(titanic.model)
```

---
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 